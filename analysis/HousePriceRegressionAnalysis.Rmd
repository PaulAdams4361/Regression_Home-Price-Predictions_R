---
title: "Regression Analysis of the Ames, Iowa Dataset"
author: "Stuart Miller, Paul Adams, and Chance Robinson"
date: |
  Master of Science in Data Science, Southern Methodist University, USA
lang: en-US
class: man
# figsintext: true
numbersections: true
encoding: UTF-8
bibliography: references.bib
biblio-style: apalike
output:
  bookdown::pdf_document2:
     citation_package: natbib
     keep_tex: true
     toc: false
header-includes:
   - \usepackage{amsmath}
   - \usepackage[utf8]{inputenc}
   - \usepackage[T1]{fontenc}
   - \usepackage{setspace}
   - \usepackage{hyperref}
   - \onehalfspacing
   - \setcitestyle{numbers,square,super}
   - \newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
editor_options: 
  chunk_output_type: console
---

```{r, lib-read, results='hide', message=FALSE, include=FALSE, echo=FALSE}
### Compuational Setup
# libraries
library(knitr)
library(kableExtra)
library(tidyverse)
library(olsrr)
library(gridExtra)
library(caret)
library(multcomp)
library(Hmisc)

# set a random seed for repodicibility
set.seed(123)

# helper code
source('./helper/visual.R')
source('./helper/data_munging.R')
source('./helper/performance.R')

# load data
train <- read_csv('./data/train.csv')
train2 <- read_csv('./data/train.csv')
test <- read_csv('./data/test.csv')

train <- train %>% 
  filter(Neighborhood %in% c("Edwards", "BrkSide", "NAmes"))
train$Neighborhood <- as.factor(train$Neighborhood)

# create dummy variables with Neighborhood == 'Edwards' as reference
train <- get.dummies(train, "Neighborhood", reference = 'Edwards')

# remove suspect points from training data
train.mod <- train %>% filter(GrLivArea < 4000)
train2.mod <- train2 %>% filter(GrLivArea < 4000)
```




# Introduction

@Sleuth CWR Update Test

# Ames, Iowa Data Set

The Ames, Iowa Data Set describes the sale of individual residential properities from 2006-2010 in Ames, Iowa \cite{Cock}. The data was retreved from the dataset hosting site Kaggle, where it is listed under a machine learning competition named \href{https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview}{\textit{House Prices: Advanced Regression Techniques}} \cite{Kaggle2016}. The data is comprised of 37 numeric features, 43 non-numeric features and an observation index split between a training set and a testing set, which contain 1460 and 1459 observations, respectively. The response variable (`SalePrice`) is only provided for the training set. The output of a model on the test set can be submitted to the Kaggle competition for scoring the performance of the model in terms of RMSE. The first analysis models property sale prices (`SalePrice`) as the response of living room area (`GrLivArea`) of the property and neighborhood (`Neighborhood`) where it is located. **Add some details on the question 2 variables?**

# Analysis Question I

## Question of Interest

Century 21 has commissioned an analysis of this data to determine how the sale price of property is related to living room area of the property in the Edwards, Northwest Ames, and Brookside neighborhoods of Ames, IA.

## Modeling

Linear regression will be used to model sale price as a response of the living room area. From the initial exploratory data analysis, it was determined that sale prices should be log-transformed to meet the model assumptions for linearity (see section \ref{appendix:linearity}), thus improving our models fit and reducing standard error. Additionally, two observations were removed as they appeared to be from a different population than the other observations in the dataset (see section \ref{appendix:infleu-points}); therefore, analysis only considers properties with living rooms less than 3500 sq. ft. in area.

We will consider two models: the logarithm of sale price as the response of living room area (1), the reduced model, and the logarithm of sale price as the response of living room area accounting for differences in the three neighborhood of interest (Brookside, Northwest Ames, and Edwards) where Edwards will be used as the reference (2), the full model. An extra sums of square (ESS) test will be used to verify that the addition of `Neighborhood` improves the model.

**Reduced Model**

\begin{equation}
\mu \lbrace log(SalePrice) \rbrace = \beta_0 + \beta_1(LivingRoomArea) (\#eq:reduced)
\end{equation}

**Full Model**

\begin{align}
\mu \lbrace log(SalePrice) \rbrace = \beta_0 + \beta_1(LivingRoomArea) +  \beta_2(Brookside) +\beta_3(NorthwestAmes) + \nonumber\\
\beta_3(Brookside)(LivingRoomArea) + \beta_4(NorthwestAmes)(LivingRoomArea) (\#eq:full)
\end{align}

The ESS test provides convincing evidence that the interaction terms are useful for the model (p-value < 0.0001); thus, we will continue with the full model.

```{r, ESS, echo=FALSE}
# full model formula
model.formula = log(SalePrice) ~ (GrLivArea) + 
     Neighborhood_BrkSide + 
     Neighborhood_NAmes +
     (GrLivArea) * Neighborhood_BrkSide + 
     (GrLivArea) * Neighborhood_NAmes
# reduced model formula
model.reduced.formula = log(SalePrice) ~ (GrLivArea) + 
     Neighborhood_BrkSide + 
     Neighborhood_NAmes

# fit models
model <- lm(formula = model.formula, data = train.mod)
model.reduced <- lm(formula = model.reduced.formula, data = train.mod)
# ESS test on models
anova(model.reduced, model)

```

## Model Assumptions Assessment

The following assessments for model assumptions are made based on Figure \@ref(fig:diag-plots) and Figure \@ref(fig:scatter-plots):

* The residuals of the model appear to be approximately normally distrubited based on the QQ plot of the residuals and histogram of the residuals, suggesting the assumption of normality is met.
* No patterns are evident in the scatter plots of residuals and studentized residuals vs predicted value, suggesting the assumption of constant variance is met.
* While some observations appear to be influential and have high leverage, removing these observations does not have a significant impact on the result of the model fit.
* Based on the scatter plot of the log transform of `SalePrice` vs `GrLivArea`, it appears that a linear model is reasonable (see section \ref{appendix:linearity}).

The sampling procedure is not known. We will assume the independence assumption is met.

```{r, diag-plots, echo=FALSE, fig.width=5, fig.height=5, fig.align='center', fig.cap='Diagnostic Plots', out.width = '45%', fig.pos="htbp", fig.show = 'hold'}
# option 'htbp' is used to lock the position of the image
# option 'hold' is used to arrange images side-by-side

# create plots of residuals
basic.fit.plots(train.mod, model)
# create leverage / outlier plot
ols_plot_resid_lev(model)
```

## Comparing Competing Models

The two models were trained and validated on the training dataset using 10-fold cross validation. The table below summerizes the performance of the models with RMSE, adjusted $R^2$, and PRESS. These results show that the full model is an improvement over the reduced model, which is consistent with the result of the ESS test.

```{r, cross-validation, results='hide', message=FALSE, include=FALSE, echo=FALSE}
## cross validate the full model

# Set up repeated k-fold cross-validation
train.control <- trainControl(method = "cv", number = 10)
# Train the model
model.cv <- train(model.formula, 
                    data = train.mod,
                    method = 'lm',
                    trControl = train.control)
# print model summary
model.cv

# get the CV results
res <- model.cv$results

# get cross-validated PRESS statistic
PCV <- PRESS.cv(model.cv)

## cross validate the reduced model

# Set up repeated k-fold cross-validation
train.control <- trainControl(method = "cv", number = 10)
# Train the model
model.reduced.cv <- train(model.reduced.formula, 
                    data = train.mod,
                    method = 'lm',
                    trControl = train.control)
# print model summary
model.reduced.cv

# get the CV results
res.red <- model.reduced.cv$results

# get cross-validated PRESS statistic
PCV.red <- PRESS.cv(model.reduced.cv)
```

```{r, echo=FALSE}
# print accuracy metrics to md table
kable(data.frame('Model' = c('Full Model', 'Reduced Model'), 
                 'RMSE'=c(res$RMSE, res.red$RMSE),
                 'CV Press'=c(PCV, PCV.red),
                 'Adjused R Squared'=c(res$Rsquared, res.red$Rsquared)),
      "latex", booktabs = T)  %>%
  kable_styling(position = "center")
```

## Parameters

The following table summerizes the parameter estimates for the full model.

```{r, echo=FALSE}
# extract the model estimates from the model summary
sm <- summary(model)
sm.coe <- sm$coefficients
# get the CIs for the coefficients
model.conf <- confint(model)

# print model estimates to md / latex table
kable(data.frame('Parameter' = c('Intercept', 'GrLivArea', 
                                 'Neighborhood_BrkSide', 'Neighborhood_NAmes', 
                                 'GrLivArea:Neighborhood_BrkSide', 'GrLivArea:Neighborhood_NAmes '), 
                 'Estimate'=c(sm.coe[[1]],sm.coe[[2]],sm.coe[[3]],sm.coe[[4]],sm.coe[[5]],sm.coe[[6]]),
                 'CI Lower' = c(model.conf[[1]],model.conf[[2]],model.conf[[3]],
                                model.conf[[4]],model.conf[[5]],model.conf[[6]]),
                 'CI Upper' = c(model.conf[[1,2]],model.conf[[1,2]],model.conf[[3,2]],
                                model.conf[[4,2]],model.conf[[5,2]],model.conf[[6,2]])),
      "latex", booktabs = T)  %>%
  kable_styling(position = "center")
```

Where `Intercept` is $\beta_0$, `GrLivArea` is $\beta_1$, `Neighborhood_BrkSide` is $\beta_2$, `Neighborhood_NAmes` is $\beta_3$, `GrLivArea:Neighborhood_BrkSide` is $\beta_4$, and `GrLivArea:Neighborhood_NAmes` is $\beta_5$

## Model Interpretation

We estimate that for increase in 100 sq. ft., there is associated multiplicative increase in median price of

* 1.055 for the Edwards neighborhood with a 95% confidence interval of [1.044 , 1.066]
* 1.033 for the Northwest Ames neighorhood with a 95% confidence interval of [1.026 , 1.040]
* 1.077 for the Brookside neighorhood with a 95% confidence interval of [1.063 , 1.090]

Since the sampling procedure is not known and this is an observational study, the results only apply to this data.

## Conclusion

```{r, echo=FALSE, results='hide'}
# summary of model to get overall test
summary(lm(model.formula, data = train.mod))
```


In response to the analysis commissioned by Century 21, the log transform of property sale price was modeled as a linear response to the property living room area for residential properties in Ames, IA. It was determined that it was necessary to include interaction terms to allow for the influence of neighborhood on sale price. Based on the model, there is strong evidence of an associated multiplicative increase in median sale price for an increase in living room area (p-vlue < 0.0001, overall F-test).

# Analysis Question II

## Question of Interest

Century 21 has commissioned a second analysis using the same data set, expanded to include as many of the 80 total features, plus the index split, as required to determine the sale price of residential properties across all neighborhoods of Ames, Iowa, beyond only the three - Edwards, Northwest Ames, and Brookside - previously commissioned for analysis.

## Modeling
Restatement of Problem

Through analyzing our variable selection and cross-validation processes - along with our nascant domain knowledge of residential real estate - we ultimately arrived at a multiple linear regression model featuring 11 linear predictor variables and three interaction terms. Specifically, our variable selection process included direct analysis of a correlation plot and a correlation matrix as well as performing forward selection, backward elimination, and stepwise regression. Because many variables contained zeroes that were significant to the factor (for example, bathrooms), we elected to perform logarithmic transformation only to variable `LotArea`, representative of the area of the real estate lot.

In addition to the transformation of `LotArea`, we imputed NA values for 19 variables using a combination of the Data Dictionary provided by Century 21 as well as our domain knowledge.

Type of selection

Forward Selection

Forward selection is a variable selection methodology that begins with a constant mean and adds explanatory variables one-by-one until no further additonal predictor variables significantly improve the model's fit. This employess the "F-to-enter" method from the extra-sum-of-squares F-statistic. This was the first method we employed. For this process, we provided the test a starting model with no predictor variables and a model from which terms can be selected, which included all predictor variables available. The process worked forward with selecting one parameter. The suggested model is output below.

```{r, echo=F}
# Count NAs, impute where needed
na_count <- sapply(train2, function(cnt) sum(length(which(is.na(cnt)))))
train2$LotFrontage[is.na(train2$LotFrontage)] <- 0
train2$Alley[is.na(train2$Alley)] <- "None"
train2$MasVnrType[is.na(train2$MasVnrType)] <- "None"
train2$MasVnrArea[is.na(train2$MasVnrArea)] <- 0
train2$BsmtQual[is.na(train2$BsmtQual)] <- 0
train2$BsmtCond[is.na(train2$BsmtCond)] <- 0
train2$BsmtExposure[is.na(train2$BsmtExposure)] <- 0
train2$BsmtFinType1[is.na(train2$BsmtFinType1)] <- 0
train2$BsmtFinType2[is.na(train2$BsmtFinType2)] <- 0
train2$Electrical[is.na(train2$Electrical)] <- "SBrkr"
train2$FireplaceQu[is.na(train2$FireplaceQu)] <- "None"
train2$GarageType[is.na(train2$GarageType)] <- "None"
train2$GarageYrBlt[is.na(train2$GarageYrBlt)] <- mean(train2$YearBuilt)
train2$GarageFinish[is.na(train2$GarageFinish)] <- "None"
train2$GarageQual[is.na(train2$GarageQual)] <- "None"
train2$GarageCond[is.na(train2$GarageCond)] <- "None"
train2$PoolQC[is.na(train2$PoolQC)] <- "None"
train2$Fence[is.na(train2$Fence)] <- "None"
train2$MiscFeature[is.na(train2$MiscFeature)] <- "None"

# Log-transform lot area
train2$LotArea <- log(train2$LotArea)
```

```{r, echo=T}
# Initial Model - Forward Selection
model2.forward.Start <- lm(log(SalePrice)~1,data = train2)

# All Variables Model - Forward Selection
model2.Allvar <- lm(log(SalePrice) ~ Id + MSSubClass + MSZoning + LotFrontage + LotArea + Street + 
                      Alley + LotShape + LandContour + Utilities + LotConfig + LandSlope +
                      Neighborhood + Condition1 + Condition2 + BldgType + HouseStyle + OverallQual +
                      OverallCond + YearBuilt + YearRemodAdd + RoofStyle + RoofMatl + Exterior1st +
                      Exterior2nd + MasVnrType + MasVnrArea + ExterQual + ExterCond + Foundation +
                      BsmtQual + BsmtCond + BsmtExposure + BsmtFinType1 + BsmtFinSF1 + BsmtFinType2 
                    + BsmtFinSF2 + BsmtUnfSF + TotalBsmtSF + Heating + HeatingQC + CentralAir + 
                      Electrical + `1stFlrSF` + `2ndFlrSF` + LowQualFinSF + GrLivArea + BsmtFullBath
                    + BsmtHalfBath + FullBath + HalfBath + BedroomAbvGr + KitchenAbvGr + KitchenQual
                    + TotRmsAbvGrd + Functional + Fireplaces + FireplaceQu + GarageType +
                      GarageYrBlt + GarageFinish + GarageCars + GarageArea + GarageQual + GarageCond
                    + PavedDrive + WoodDeckSF + OpenPorchSF + EnclosedPorch + `3SsnPorch` +
                      ScreenPorch + PoolArea + PoolQC + Fence + MiscFeature + MiscVal + MoSold +
                      YrSold + SaleType + SaleCondition, data = train2
                    )

model2.Forward <- stepAIC(model2.forward.Start, direction = "forward", trace = F, scope = formula(model2.Allvar))

summary(model2.Forward)
model2.Forward$anova

final.Forward.Model <- lm(log(SalePrice) ~ OverallQual + Neighborhood + GrLivArea + BsmtFinType1 + 
                  GarageCars + OverallCond + RoofMatl + TotalBsmtSF + YearBuilt + 
                  LotArea + Condition2 + MSZoning + BsmtUnfSF + SaleCondition + 
                  Functional + CentralAir + Condition1 + KitchenQual + Fireplaces + 
                  Exterior1st + Heating + ScreenPorch + YearRemodAdd + BsmtQual + 
                  PoolQC + Foundation + KitchenAbvGr + SaleType + GarageArea + 
                  HeatingQC + BsmtExposure + EnclosedPorch + WoodDeckSF + LotConfig + 
                  Street + BsmtFullBath + PoolArea + LandSlope + GarageCond + 
                  HalfBath + FullBath + TotRmsAbvGrd + `3SsnPorch` + ExterCond + 
                  GarageQual + GarageYrBlt + Utilities, data = train2
                  )
```


Backward Elimination

Backward elimination is a variable selection methodology that begins with all possible predictor variables and works backward, eliminating variables using all possible combinations until only the best for the fit are provided. This employess the "F-to-remove" method from the extra-sum-of-squares F-statistic. For this process, we provided the test a model with all available predictor variables from which insignificant variables were eliminated. The output is as follows below.
```{r, echo=F}
# Backward Elimination
model2.Backward <- stepAIC(model2.Allvar, direction = "backward", trace = F)
summary(model2.Backward)
model2.Backward$anova
```

Stepwise Regression

Stepwise regression is a variable selection methodology that performs one step of forward selection for each step of backward elimination. The steps are repeated, concurrently, until no further predictor variables can be added or removed. This is the third model approach we used. The suggested model is output below.

```{r, echo=F}
# Stepwise Selection
stepwise.full.model4 <- stepAIC(model2.forward.Start, direction = "both", trace = F, scope = formula(model2.Allvar))

summary(stepwise.full.model4)
stepwise.full.model4$anova
```

Custom Variable Selection 

To develop the custom model, we employed a combination of a correlation matrix for quantitative data, analysis of the summarization of the suggested model from stepwise selection, and through direct analysis of the pairs plots. As previously mentioned, our final model included 11 linear terms and three interaction terms. We removed all variables suggested to be removed by the stepwise regression and backward elimination tests, then reprocessed the updated models until forward selection, backward elimination, and stepwise regression were in agreeance with respect to the linear terms. Once this trial-and-error process was completed, we added interaction terms based on domain knowledge and re-applied the forward selection, backward elimination, and stepwise regression methods until only significant terms - both linear and interactive - remained. We then used graphical analysis to visually confirm interaction between the interactive terms remaining.

```{r, echo=F}

train2.numeric <- dplyr::select_if(train2, is.numeric) %>% data.frame()

flattenCorrMatrix <- function(cormat, pmat) {
  ut <- upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
    )
}

#See what variables are correlated with eachother, p-values
correlation.matrix <- rcorr(as.matrix(train2.numeric))
corDF <- data.frame(flattenCorrMatrix(correlation.matrix$r, correlation.matrix$P))

#Order the correlation matrix to show the highest correlated
data.frame(corDF[order(-corDF$cor),])
quantDataModel <- corDF[which(corDF$cor >= 0.5),]

fitFull.all4.old <- lm(log(SalePrice) ~ (BsmtUnfSF) + CentralAir + HalfBath + KitchenQual + Neighborhood + OverallCond + OverallQual + RoofMatl + `1stFlrSF` + `2ndFlrSF` + YearBuilt + MSZoning:Neighborhood + OverallQual:Neighborhood + YearBuilt:Neighborhood, data = train2)
summary(fitFull.all4.old)
######################################################################
###We need the graphs for interactive terms to display interaction####
######################################################################
```

## Model Assumption Assessment

Address each assumption

## Comparing Competing Models

* Adj $R^2$
The final adjusted $R^2$ of our model is 0.8916

* CV Press
Our cross-validated PRESS statistic output is below.
```{r, echo=F}
# Set up repeated k-fold cross-validation
train.control2 <- trainControl(method = "cv", number = 10)
# Train the model
model.cv2 <- train(log(SalePrice) ~ (BsmtUnfSF) + CentralAir + HalfBath + KitchenQual + Neighborhood
                   + OverallCond + OverallQual + RoofMatl + `1stFlrSF` + `2ndFlrSF` + YearBuilt +
                     MSZoning:Neighborhood + OverallQual:Neighborhood + YearBuilt:Neighborhood, 
                    data = train2,
                    method = 'lm',
                    trControl = train.control2)

# print model summary
model.cv2

# get the CV results
res <- model.cv2$results

# get cross-validated PRESS statistic
PCV <- PRESS.cv(model.cv2)
```
* Kaggle score

## Conclusion

A short summary of the analysis

# Appendix

## Checking for Linearity in `SalePrice` vs `GrLivArea`

\label{appendix:linearity}

The scatter plot in Figure \@ref(fig:scatter-plot) shows relationship of `SalePrice` vs `GrLivArea` for all three neighborhoods of interest to Century 21. Based on this plot, it does not appear that this relationship meets the assumptions of linear regression, specifically the constant varaince assumption. The response will be transformed to attempt to handle the changing variance.

```{r, scatter-plot, echo=FALSE, fig.width=5, fig.height=5, fig.align='center', fig.cap='Scatter Plot of Sale Price vs Living Room Area', out.width = '45%', fig.pos="htbp"}

# scatter plot of observations from all three neighborhoods
train %>% 
  ggplot(aes(x = (GrLivArea), y = (SalePrice))) +
  geom_point(alpha = 0.3) +
  labs(title = 'Log of Sale Price vs Living Room Area', 
       y = 'Log of Sale Price', x = 'Living Room Area (sq. ft.)')
```

The images below show the scatter plots of log sale price vs living room area (Figure \@ref(fig:scatter-plots)). In the image on the right, the scatter plot is shown for each neighborhood. In the image on the left the observations for all three neighborhoods are included. In all cases, a linear model appears to be reasonable to model this data.

```{r, scatter-plots, echo=FALSE, fig.width=5, fig.height=5, fig.align='center', fig.cap='Scatter Plots of Log of Sale Price vs Living Room Area', out.width = '45%', fig.pos="htbp", fig.show='hold'}

# plots of log of sale price ~ living room area

# create scatter plot for northwest ames
regplot.names <- train %>% filter(Neighborhood == 'NAmes') %>%
  ggplot(aes(x = (GrLivArea), y = log(SalePrice))) +
  geom_point(alpha = 0.3) +
  ylim(10, 13) +
  xlim(0, 3500) +
  labs(subtitle = 'Northwest Ames', 
       y = 'Log of Sale Price', x = 'Living Room Area (sq. ft.)')

# create scatter plot for edwards
regplot.ed <- train %>%
  filter(GrLivArea < 4000) %>%
  filter(Neighborhood == 'Edwards') %>%
  ggplot(aes(x = (GrLivArea), y = log(SalePrice))) +
  geom_point(alpha = 0.3) +
  ylim(10, 13) +
  xlim(0, 3500) +
  labs(subtitle = 'Edwards', 
       y = 'Log of Sale Price', x = 'Living Room Area (sq. ft.)')

# create regression plot for brookside
regplot.brk <- train %>% filter(Neighborhood == 'BrkSide') %>%
  ggplot(aes(x = (GrLivArea), y = log(SalePrice))) +
  geom_point(alpha = 0.3) +
  ylim(10, 13) +
  xlim(0, 3500) +
  labs(subtitle = 'Brook Side', 
       y = 'Log of Sale Price', x = 'Living Room Area (sq. ft.)')

# add the scatter plots for the neighborhood into a single plot
grid.arrange(regplot.names,regplot.ed,regplot.brk, nrow = 2,
             top = 'Regression Plots for Neighborhoods')

# scatter plot of observations from all three neighborhoods
train %>% 
  filter(GrLivArea < 4000) %>%
  ggplot(aes(x = (GrLivArea), y = log(SalePrice))) +
  geom_point(alpha = 0.3) +
  labs(title = 'Log of Sale Price vs Living Room Area', 
       y = 'Log of Sale Price', x = 'Living Room Area (sq. ft.)')
```


## Analysis of Influential points

\label{appendix:infleu-points}

The two outlying observations with living room areas greater than 4000 sq. ft. appear to be from a different distribution than the main dataset. Since these are partial sales, it is possible that the sale prices do not reflect market value. For this reason, we will limit the analysis to properities with less than 3500 sq. ft.  \@ref(fig:infleu-points)

```{r, infleu-points, echo=FALSE, fig.width=5, fig.height=5, fig.align='center', fig.cap='Influential Points', out.width = '50%', fig.pos="htbp"}

# scatter plot of observations from all three neighborhoods with labeling by `SaleCondition`
train %>% ggplot(aes(x = (GrLivArea), y = log(SalePrice))) +
  geom_point(alpha = 0.3) +
  labs(title = 'Log of Sale Price vs Living Room Area', 
       y = 'Log of Sale Price', x = 'Living Room Area (sq. ft.)') +
  geom_text(aes(label = ifelse((log(GrLivArea) > 7.75 & log(SalePrice) > 11) |
                                 (log(SalePrice) > 12.45),
                               SaleCondition, '')), hjust=0, vjust=0)
```

\newpage

## R Code For Analysis 1

```r
### Compuational Setup
# libraries
library(knitr)
library(kableExtra)
library(tidyverse)
library(olsrr)
library(gridExtra)
library(caret)
library(multcomp)

# load data
train <- read_csv('./data/train.csv')
test <- read_csv('./data/test.csv')

# set a random seed for repodicibility
set.seed(123)

### Helper Code

#' Print Typical Regression Fit Plots
#' 
#' @description
#' Plots QQ plot of residuals, histogram of residuals,
#' residuals vs predicted values, and studentized 
#' residuals vs predicted values. Depends on tidyverse
#' and gridExtra packages being loaded.
#'
#' @param data The true values corresponding to the input.
#' @param model The predicted/fitted values of the model.
basic.fit.plots <- function(data, model) {
	
	# depends on
	require(tidyverse)
	require(gridExtra)

	# get predicted values
	data$Predicted <- predict(model, data)
	# get residuals
	data$Resid <- model$residuals
	# get studentized residuals
	data$RStudent <- rstudent(model = model)

	# create qqplot of residuals with reference line
	qqplot.resid <- data %>% 
	  ggplot(aes(sample = Resid)) +
	  geom_qq() + geom_qq_line() +
	  labs(subtitle = 'QQ Plot of Residuals',
	       x = 'Theoretical Quantile',
	       y = 'Acutal Quantile')
	
	# create histogram of residuals
	hist.resid <- data %>% 
	  ggplot(aes(x = Resid)) +
	  geom_histogram(bins = 15) + 
	  labs(subtitle = 'Histogram of Residuals',
	       x = 'Residuals',
	       y = 'Count')

	# create scatter plot of residuals vs predicted values
	resid.vs.pred <- data %>% 
	  ggplot(aes(x = Predicted, y = Resid)) +
	  geom_point() +
	  geom_abline(slope = 0) + 
	  labs(subtitle = 'Residuals vs Prediction',
	       x = 'Predicted Value',
	       y = 'Residual')

	# create scatter plot of studentized 
	# residuals vs predicted values
	rStud.vs.pred <- data %>% 
	  ggplot(aes(x = Predicted, y = RStudent)) +
	  geom_point() +
	  geom_abline(slope = 0) + 
  	  geom_abline(slope = 0, intercept = -2) + 
  	  geom_abline(slope = 0, intercept = 2) + 
	  labs(subtitle = 'RStudent vs Prediction',
	       x = 'Predicted Value',
	       y = 'RStudent')
	
	# add all four plots to grid as
	# qqplot           histogram
	# resid vs pred    RStud vs pred
	grid.arrange(qqplot.resid,
		     hist.resid,
		     resid.vs.pred,
		     rStud.vs.pred, 
		     nrow = 2,
		     top = 'Fit Assessment Plots')
}

#' Creates dummy variables (columns) for given column
#'
#' @param data A dataframe.
#' @param column A categorical column in data.
#' @param reference A value in the column to use a reference.
#' @param as.onehot Set to TRUE to use onehot encoding.
#'
get.dummies <- function(data, column, reference, as.onehot = FALSE) {
  # get the levels of the factor in column
  lev <- levels(data[[column]])
  # do not remove reference for onehot encoding
  if (!as.onehot) {
    # remove the reference value
    lev <- lev[lev != reference]
  }
  # add encodings
  for (fct in lev){
    new_col <- paste(column, fct, sep = '_')
    data[new_col] <- as.numeric(data[, column] == fct)
    print(new_col)
  }
  data
}

#' Calculates PRESS from `caret` CV model
#'
#' @param model.cv Calculates press from a model 
#' produced by `caret`
#'
PRESS.cv <- function(model.cv) {
  meanN <- 0
  folds <- model.cv$control$index
  for (i in seq(1:length(folds))){
    meanN <- meanN + length(folds[[i]])
  }
  meanN <- meanN / length(folds)
  meanN * ((model.cv$results$RMSE)^2)
}

### plots of log of sale price ~ living room area

# create scatter plot for northwest ames
regplot.names <- train %>% filter(Neighborhood == 'NAmes') %>%
  ggplot(aes(x = (GrLivArea), y = log(SalePrice))) +
  geom_point(alpha = 0.3) +
  ylim(10, 13) +
  xlim(0, 3500) +
  labs(subtitle = 'Northwest Ames', 
       y = 'Log of Sale Price', x = 'Living Room Area (sq. ft.)')

# create scatter plot for edwards
regplot.ed <- train %>%
  filter(GrLivArea < 4000) %>%
  filter(Neighborhood == 'Edwards') %>%
  ggplot(aes(x = (GrLivArea), y = log(SalePrice))) +
  geom_point(alpha = 0.3) +
  ylim(10, 13) +
  xlim(0, 3500) +
  labs(subtitle = 'Edwards', 
       y = 'Log of Sale Price', x = 'Living Room Area (sq. ft.)')

# create regression plot for brookside
regplot.brk <- train %>% filter(Neighborhood == 'BrkSide') %>%
  ggplot(aes(x = (GrLivArea), y = log(SalePrice))) +
  geom_point(alpha = 0.3) +
  ylim(10, 13) +
  xlim(0, 3500) +
  labs(subtitle = 'Brook Side', 
       y = 'Log of Sale Price', x = 'Living Room Area (sq. ft.)')

# add the scatter plots for the neighborhood into a single plot
grid.arrange(regplot.names,regplot.ed,regplot.brk, nrow = 2,
             top = 'Regression Plots for Neighborhoods')

# scatter plot of observations from all three neighborhoods
train %>% 
  filter(GrLivArea < 4000) %>%
  ggplot(aes(x = (GrLivArea), y = log(SalePrice))) +
  geom_point(alpha = 0.3) +
  labs(title = 'Log of Sale Price vs Living Room Area', 
       y = 'Log of Sale Price', x = 'Living Room Area (sq. ft.)')

### Filter data for analysis 1

train <- train %>% 
  filter(Neighborhood %in% c("Edwards", "BrkSide", "NAmes"))
train$Neighborhood <- as.factor(train$Neighborhood)

# create dummy variables with Neighborhood == 'Edwards' as reference
train <- get.dummies(train, "Neighborhood", reference = 'Edwards')

# remove suspect points from training data
train.mod <- train %>% filter(GrLivArea < 4000)

#### Extra Sum of Squares

# full model formula
model.formula = log(SalePrice) ~ (GrLivArea) + 
     Neighborhood_BrkSide + 
     Neighborhood_NAmes +
     (GrLivArea) * Neighborhood_BrkSide + 
     (GrLivArea) * Neighborhood_NAmes
# reduced model formula
model.reduced.formula = log(SalePrice) ~ (GrLivArea) + 
     Neighborhood_BrkSide + 
     Neighborhood_NAmes

# fit models
model <- lm(formula = model.formula, data = train.mod)
model.reduced <- lm(formula = model.reduced.formula, data = train.mod)
# ESS test on models
anova(model.reduced, model)

### Assessment plots

# create plots of residuals
basic.fit.plots(train.mod, model)
# create leverage / outlier plot
ols_plot_resid_lev(model)

### cross validation

## cross validate the full model

# Set up repeated k-fold cross-validation
train.control <- trainControl(method = "cv", number = 10)
# Train the model
model.cv <- train(model.formula, 
                    data = train.mod,
                    method = 'lm',
                    trControl = train.control)
# print model summary
model.cv

# get the CV results
res <- model.cv$results

# get cross-validated PRESS statistic
PCV <- PRESS.cv(model.cv)

## cross validate the reduced model

# Set up repeated k-fold cross-validation
train.control <- trainControl(method = "cv", number = 10)
# Train the model
model.reduced.cv <- train(model.reduced.formula, 
                    data = train.mod,
                    method = 'lm',
                    trControl = train.control)
# print model summary
model.reduced.cv

# get the CV results
res.red <- model.reduced.cv$results

# get cross-validated PRESS statistic
PCV.red <- PRESS.cv(model.reduced.cv)

# print accuracy metrics to md table
kable(data.frame('Model' = c('Full Model', 'Reduced Model'), 
                 'RMSE'=c(res$RMSE, res.red$RMSE),
                 'CV Press'=c(PCV, PCV.red),
                 'Adjused R Squared'=c(res$Rsquared, res.red$Rsquared)),
      "latex", booktabs = T)  %>%
  kable_styling(position = "center")

### get the parameters from the CV'ed model

# extract the model estimates from the model summary
sm <- summary(model)
sm.coe <- sm$coefficients
# get the CIs for the coefficients
model.conf <- confint(model)

# print model estimates to md / latex table
# extract the params and put into a dataframe
kable(data.frame('Parameter' = c('Intercept', 'GrLivArea', 
                                 'Neighborhood_BrkSide', 'Neighborhood_NAmes', 
                                 'GrLivArea:Neighborhood_BrkSide', 
                                 'GrLivArea:Neighborhood_NAmes '), 
                 'Estimate'=c(sm.coe[[1]],sm.coe[[2]],sm.coe[[3]],
                              sm.coe[[4]],sm.coe[[5]],sm.coe[[6]]),
                 'CI Lower' = c(model.conf[[1]],model.conf[[2]],model.conf[[3]],
                                model.conf[[4]],model.conf[[5]],model.conf[[6]]),
                 'CI Upper' = c(model.conf[[1,2]],model.conf[[1,2]],model.conf[[3,2]],
                                model.conf[[4,2]],model.conf[[5,2]],model.conf[[6,2]])),
      "latex", booktabs = T)  %>%
  kable_styling(position = "center")

# summary of model to get overall test
summary(lm(model.formula, data = train.mod))

## Calculate CIs of slopes not in standard table

# get CI for Northwest Ames
confint(glht(model, linfct = "GrLivArea + GrLivArea:Neighborhood_NAmes = 1"))

# get CI for Brookside
confint(glht(model, linfct = "GrLivArea + GrLivArea:Neighborhood_BrkSide = 1"))

```

## R Code For Analysis 2

Include "well commented" `code` in the appendex!

```r
train %>% ggplot(aes(x = (GrLivArea), y = log(SalePrice))) +
  geom_point(alpha = 0.3) +
  labs(title = 'Log of Sale Price vs Living Room Area', 
       y = 'Log of Sale Price', x = 'Living Room Area') +
  geom_text(aes(label = ifelse((log(GrLivArea) > 7.75 & log(SalePrice) > 11) |
                                 (log(SalePrice) > 12.45),
                               SaleCondition, '')), hjust=0, vjust=0)
```

# References
