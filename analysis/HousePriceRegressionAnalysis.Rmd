---
title: "Title"
author: "Stuart Miller, Paul Adams, and Chance Robinson"
date: |
  Master of Science in Data Science, Southern Methodist University, USA
lang: en-US
class: man
# figsintext: true
numbersections: true
encoding: UTF-8
bibliography: references.bib
biblio-style: apalike
output:
  bookdown::pdf_document2:
     citation_package: natbib
     keep_tex: true
     toc: false
header-includes:
   - \usepackage{amsmath}
   - \usepackage[utf8]{inputenc}
   - \usepackage[T1]{fontenc}
   - \usepackage{setspace}
   - \onehalfspacing
   - \setcitestyle{round}
   - \newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
editor_options: 
  chunk_output_type: console
---

```{r, lib-read, results='hide', message=FALSE, include=FALSE, echo=FALSE}
### Compuational Setup
# libraries
library(knitr)
library(tidyverse)
library(olsrr)
library(gridExtra)
library(caret)
library(multcomp)

# set a random seed for repodicibility
set.seed(123)
getwd()
# helper code
source('./helper/visual.R')
source('./helper/data_munging.R')
source('./helper/performance.R')

# load data
read.csv('./data/test.csv')
train <- read_csv('./data/train.csv')
test <- read_csv('./data/test.csv')

train <- train %>% 
  filter(Neighborhood %in% c("Edwards", "BrkSide", "NAmes"))
train$Neighborhood <- as.factor(train$Neighborhood)

# create dummy variables with Neighborhood == 'Edwards' as reference
train <- get.dummies(train, "Neighborhood", reference = 'Edwards')

# remove suspect points from training data
train.mod <- train %>% filter(GrLivArea < 4000)

```

# Introduction

@Sleuth

# Ames, Iowa Data


@Kaggle2016

# Analysis Question I

## Question of Interest

Restatement of the problem

## Modeling

TODO: Build and fit the model

We will consider two models: (1) the logarithm of sale price as the response of living room area and (2) the logarithm of sale price as the response of living room area accounting for differences in the three neighborhood of interest (Brookside, Northwest Ames, and Edwards) where Edwards will be used as the reference.

**Reduced Model**

\begin{equation}
\mu \lbrace log(SalePrice) \rbrace = \beta_0 + \beta_1(LivingRoomArea) (\#eq:reduced)
\end{equation}

**Full Model**

\begin{align}
\mu \lbrace log(SalePrice) \rbrace = \beta_0 + \beta_1(LivingRoomArea) +  \beta_2(Brookside) +\beta_3(NorthwestAmes) + \nonumber\\
\beta_3(Brookside)(LivingRoomArea) + \beta_4(NorthwestAmes)(LivingRoomArea) (\#eq:full)
\end{align}

We will use an extra sums of square test to verify that the interaction terms are useful for the model. The ESS test provides convincing evidence that the interaction terms are useful for the model (p-value < 0.0001); thus, we will continue with the full model.

```{r, ESS, echo=FALSE}
# full model formula
model.formula = log(SalePrice) ~ (GrLivArea) + 
     Neighborhood_BrkSide + 
     Neighborhood_NAmes +
     (GrLivArea) * Neighborhood_BrkSide + 
     (GrLivArea) * Neighborhood_NAmes
# reduced model formula
model.reduced.formula = log(SalePrice) ~ (GrLivArea) + 
     Neighborhood_BrkSide + 
     Neighborhood_NAmes

# fit models
model <- lm(formula = model.formula, data = train.mod)
model.reduced <- lm(formula = model.reduced.formula, data = train.mod)
# ESS test on models
anova(model.reduced, model)

```

## Model Assumption Assessment

Address each assumption

```{r, diag-plots, echo=FALSE, fig.width=5, fig.height=5, fig.align='center'}
basic.fit.plots(train.mod, model)
#ols_plot_resid_lev(model)
#ols_plot_cooksd_bar(model)
```

## Comparing Competing Models

* Adj $R^2$
* CV Press

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|}
\hline
Model         & Adj $R^2$ & CV PRESS \\ \hline
Reduced Model & 0.5       & 2000     \\ \hline
Full Model    & 0.7       & 1500     \\ \hline
\end{tabular}
\end{table}

```{r, cross-validation, results='hide', message=FALSE, include=FALSE, echo=FALSE}
# Set up repeated k-fold cross-validation
train.control <- trainControl(method = "cv", number = 10)
# Train the model
model.cv <- train(model.formula, 
                    data = train.mod,
                    method = 'lm',
                    trControl = train.control)
# print model summary
model.cv

# get the CV results
res <- model.cv$results

# get cross-validated PRESS statistic
PCV <- PRESS.cv(model.cv)


```

```{r, echo=FALSE}
# print accuracy metrics to md table
kable(data.frame('RMSE'=res$RMSE,
           'CV Press'=PCV,
           'Adjused R Squared'=res$Rsquared))
```


## Parameters

* Estimates
* Influential points
* Residual plots

## Conclusion

A short summary of the analysis

# Analysis Question II

## Question of Interest

Restatement of the problem

## Modeling

Type of selection

## Model Assumption Assessment

Address each assumption

## Comparing Competing Models

* Adj $R^2$
* CV Press
* Kaggle score

## Conclusion

A short summary of the analysis

# Appendix

Include "well commented" `code` in the appendex!

# References