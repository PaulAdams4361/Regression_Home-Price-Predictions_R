---
title: "Regression Analysis of the Ames, Iowa Dataset"
author: "Stuart Miller, Paul Adams, and Chance Robinson"
date: |
  Master of Science in Data Science, Southern Methodist University, USA
lang: en-US
class: man
# figsintext: true
numbersections: true
encoding: UTF-8
bibliography: references.bib
biblio-style: apalike
output:
  bookdown::pdf_document2:
     citation_package: natbib
     keep_tex: true
     toc: false
header-includes:
   - \usepackage{amsmath}
   - \usepackage[utf8]{inputenc}
   - \usepackage[T1]{fontenc}
   - \usepackage{setspace}
   - \usepackage{hyperref}
   - \onehalfspacing
   - \setcitestyle{numbers,square,super}
   - \newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
editor_options: 
  chunk_output_type: console
---

```{r, lib-read, results='hide', message=FALSE, include=FALSE, echo=FALSE}
### Compuational Setup
# libraries
library(knitr)
library(kableExtra)
library(tidyverse)
library(olsrr)
library(gridExtra)
library(caret)
library(multcomp)
library(Hmisc)

# set a random seed for repodicibility
set.seed(123)

# helper code
source('./helper/visual.R')
source('./helper/data_munging.R')
source('./helper/performance.R')

# load data
train <- read_csv('./data/train.csv')

# data for analysis 2
train2 <- read_csv('./data/train.csv')
test <- read_csv('./data/test.csv')

train <- train %>% 
  filter(Neighborhood %in% c("Edwards", "BrkSide", "NAmes"))
train$Neighborhood <- as.factor(train$Neighborhood)

# create dummy variables with Neighborhood == 'Edwards' as reference
train <- get.dummies(train, "Neighborhood", reference = 'Edwards')

# remove suspect points from training data
train.mod <- train %>% filter(GrLivArea < 4000)

# data for analysis 2
train2 <- train2 %>% filter(GrLivArea < 4000)
```

# Introduction

What is the price of a home in Ames, Iowa?  Our inaugural project in the program curriculum had us competing in an online competition utilizing the linear regression techniques that we’ve learned up to this point.  Our team chose R as the preferred analysis platform as the consensus was that it had more applicable uses in industry.  The objective was to apply various predictive models in order to assess the suitability of our parameter selections in determining the sales price of a home.  The measure of accuracy was logged in terms of the Root Mean Square Error, or RMSE, as well as other comparison models such as cross-validation and the adjusted R-squared.  Our approach was limited in that we not permitted to use more advanced algorithms that we will be exposed to later on, but the exploratory data analysis and data cleaning methods that we’ve learned and made use of here will surely be of use to us in our future personal and academic endeavors.

# Ames, Iowa Data Set

The Ames, Iowa Data Set describes the sale of individual residential properities from 2006-2010 in Ames, Iowa \cite{Cock}. The data was retreved from the dataset hosting site Kaggle, where it is listed under a machine learning competition named \href{https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview}{\textit{House Prices: Advanced Regression Techniques}} \cite{Kaggle2016}. The data is comprised of 37 numeric features, 43 non-numeric features and an observation index split between a training set and a testing set, which contain 1460 and 1459 observations, respectively. The response variable (`SalePrice`) is only provided for the training set. The output of a model on the test set can be submitted to the Kaggle competition for scoring the performance of the model in terms of RMSE. The first analysis models property sale prices (`SalePrice`) as the response of living room area (`GrLivArea`) of the property and neighborhood (`Neighborhood`) where it is located. In the second analysis variable selection techniques are used to determine which explanatory varaibles are associated with `SalePrice`.

# Analysis Question I

## Question of Interest

Century 21 has commissioned an analysis of this data to determine how the sale price of property is related to living room area of the property in the Edwards, Northwest Ames, and Brookside neighborhoods of Ames, IA.

## Modeling

Linear regression will be used to model sale price as a response of the living room area. From the initial exploratory data analysis, it was determined that sale prices should be log-transformed to meet the model assumptions for linearity (see section \ref{appendix:linearity}), thus improving our models fit and reducing standard error. Additionally, two observations were removed as they appeared to be from a different population than the other observations in the dataset (see section \ref{appendix:infleu-points}); therefore, analysis only considers properties with living rooms less than 3500 sq. ft. in area.

We will consider two models: the logarithm of sale price as the response of living room area (1), the reduced model, and the logarithm of sale price as the response of living room area accounting for differences in the three neighborhood of interest (Brookside, Northwest Ames, and Edwards) where Edwards will be used as the reference (2), the full model. An extra sums of square (ESS) test will be used to verify that the addition of `Neighborhood` improves the model.

**Reduced Model**

\begin{equation}
\mu \lbrace log(SalePrice) \rbrace = \beta_0 + \beta_1(LivingRoomArea) (\#eq:reduced)
\end{equation}

**Full Model**

\begin{align}
\mu \lbrace log(SalePrice) \rbrace = \beta_0 + \beta_1(LivingRoomArea) +  \beta_2(Brookside) +\beta_3(NorthwestAmes) + \nonumber\\
\beta_3(Brookside)(LivingRoomArea) + \beta_4(NorthwestAmes)(LivingRoomArea) (\#eq:full)
\end{align}

The ESS test provides convincing evidence that the interaction terms are useful for the model (p-value < 0.0001); thus, we will continue with the full model.

```{r, ESS, echo=FALSE}
# full model formula
model.formula = log(SalePrice) ~ (GrLivArea) + 
     Neighborhood_BrkSide + 
     Neighborhood_NAmes +
     (GrLivArea) * Neighborhood_BrkSide + 
     (GrLivArea) * Neighborhood_NAmes
# reduced model formula
model.reduced.formula = log(SalePrice) ~ (GrLivArea) + 
     Neighborhood_BrkSide + 
     Neighborhood_NAmes

# fit models
model <- lm(formula = model.formula, data = train.mod)
model.reduced <- lm(formula = model.reduced.formula, data = train.mod)
# ESS test on models
anova(model.reduced, model)

```

## Model Assumptions Assessment

The following assessments for model assumptions are made based on Figure \@ref(fig:diag-plots) and Figure \@ref(fig:scatter-plots):

* The residuals of the model appear to be approximately normally distrubited based on the QQ plot of the residuals and histogram of the residuals, suggesting the assumption of normality is met.
* No patterns are evident in the scatter plots of residuals and studentized residuals vs predicted value, suggesting the assumption of constant variance is met.
* While some observations appear to be influential and have high leverage, removing these observations does not have a significant impact on the result of the model fit.
* Based on the scatter plot of the log transform of `SalePrice` vs `GrLivArea`, it appears that a linear model is reasonable (see section \ref{appendix:linearity}).

The sampling procedure is not known. We will assume the independence assumption is met.

```{r, diag-plots, echo=FALSE, fig.width=5, fig.height=5, fig.align='center', fig.cap='Diagnostic Plots', out.width = '45%', fig.pos="htbp", fig.show = 'hold'}
# option 'htbp' is used to lock the position of the image
# option 'hold' is used to arrange images side-by-side

# create plots of residuals
basic.fit.plots(train.mod, model)
# create leverage / outlier plot
ols_plot_resid_lev(model)
```

## Comparing Competing Models

The two models were trained and validated on the training dataset using 10-fold cross validation. The table below summerizes the performance of the models with RMSE, adjusted $R^2$, and PRESS. These results show that the full model is an improvement over the reduced model, which is consistent with the result of the ESS test.

```{r, cross-validation, results='hide', message=FALSE, include=FALSE, echo=FALSE}
## cross validate the full model

# Set up repeated k-fold cross-validation
train.control <- trainControl(method = "cv", number = 10)
# Train the model
model.cv <- train(model.formula, 
                    data = train.mod,
                    method = 'lm',
                    trControl = train.control)
# print model summary
model.cv

# get the CV results
res <- model.cv$results

# get cross-validated PRESS statistic
PCV <- PRESS.cv(model.cv)

## cross validate the reduced model

# Set up repeated k-fold cross-validation
train.control <- trainControl(method = "cv", number = 10)
# Train the model
model.reduced.cv <- train(model.reduced.formula, 
                    data = train.mod,
                    method = 'lm',
                    trControl = train.control)
# print model summary
model.reduced.cv

# get the CV results
res.red <- model.reduced.cv$results

# get cross-validated PRESS statistic
PCV.red <- PRESS.cv(model.reduced.cv)
```

```{r, echo=FALSE}
# print accuracy metrics to md table
kable(data.frame('Model' = c('Full Model', 'Reduced Model'), 
                 'RMSE'=c(res$RMSE, res.red$RMSE),
                 'CV Press'=c(PCV, PCV.red),
                 'Adjused R Squared'=c(res$Rsquared, res.red$Rsquared)),
      "latex", booktabs = T)  %>%
  kable_styling(position = "center")
```

## Parameters

The following table summerizes the parameter estimates for the full model.

```{r, echo=FALSE}
# extract the model estimates from the model summary
sm <- summary(model)
sm.coe <- sm$coefficients
# get the CIs for the coefficients
model.conf <- confint(model)

# print model estimates to md / latex table
kable(data.frame('Parameter' = c('Intercept', 'GrLivArea', 
                                 'Neighborhood_BrkSide', 'Neighborhood_NAmes', 
                                 'GrLivArea:Neighborhood_BrkSide', 'GrLivArea:Neighborhood_NAmes '), 
                 'Estimate'=c(sm.coe[[1]],sm.coe[[2]],sm.coe[[3]],sm.coe[[4]],sm.coe[[5]],sm.coe[[6]]),
                 'CI Lower' = c(model.conf[[1]],model.conf[[2]],model.conf[[3]],
                                model.conf[[4]],model.conf[[5]],model.conf[[6]]),
                 'CI Upper' = c(model.conf[[1,2]],model.conf[[1,2]],model.conf[[3,2]],
                                model.conf[[4,2]],model.conf[[5,2]],model.conf[[6,2]])),
      "latex", booktabs = T)  %>%
  kable_styling(position = "center")
```

Where `Intercept` is $\beta_0$, `GrLivArea` is $\beta_1$, `Neighborhood_BrkSide` is $\beta_2$, `Neighborhood_NAmes` is $\beta_3$, `GrLivArea:Neighborhood_BrkSide` is $\beta_4$, and `GrLivArea:Neighborhood_NAmes` is $\beta_5$

## Model Interpretation

We estimate that for increase in 100 sq. ft., there is associated multiplicative increase in median price of

* 1.055 for the Edwards neighborhood with a 95% confidence interval of [1.044 , 1.066]
* 1.033 for the Northwest Ames neighorhood with a 95% confidence interval of [1.026 , 1.040]
* 1.077 for the Brookside neighorhood with a 95% confidence interval of [1.063 , 1.090]

Since the sampling procedure is not known and this is an observational study, the results only apply to this data.

## Conclusion

```{r, echo=FALSE, results='hide'}
# summary of model to get overall test
summary(lm(model.formula, data = train.mod))
```


In response to the analysis commissioned by Century 21, the log transform of property sale price was modeled as a linear response to the property living room area for residential properties in Ames, IA. It was determined that it was necessary to include interaction terms to allow for the influence of neighborhood on sale price. Based on the model, there is strong evidence of an associated multiplicative increase in median sale price for an increase in living room area (p-vlue < 0.0001, overall F-test).

# Analysis Question II

## Question of Interest

Century 21 has commissioned a second analysis using the same data set, expanded to include as many of the 80 total features, plus the index split, as required to determine the sale price of residential properties across all neighborhoods of Ames, Iowa, beyond only the three - Edwards, Northwest Ames, and Brookside - previously commissioned for analysis.

## Modeling

Through analyzing our variable selection and cross-validation processes - along with our nascant domain knowledge of residential real estate - we ultimately arrived at a multiple linear regression model featuring 11 linear predictor variables and two interaction terms. Specifically, our variable selection process included direct analysis of a correlation plot and a correlation matrix as well as performing forward selection, backward elimination, and stepwise regression. 

Regarding missing data, we imputed NA values for 19 variables using a combination of the Data Dictionary provided by Century 21 as well as our domain knowledge. After building models with and without transformations applied to variables, we noted no significaznt difference in variable selection from our selection process so elected to use non-transformed predictor variables. We did, however, use the log-transformed `SalePrice` applied in the first analysis.


```{r, echo=F}
# Count NAs, impute where needed
na_count <- sapply(train2, function(cnt) sum(length(which(is.na(cnt)))))
train2$LotFrontage[is.na(train2$LotFrontage)] <- 0
train2$Alley[is.na(train2$Alley)] <- "None"
train2$MasVnrType[is.na(train2$MasVnrType)] <- "None"
train2$MasVnrArea[is.na(train2$MasVnrArea)] <- 0
train2$BsmtQual[is.na(train2$BsmtQual)] <- 0
train2$BsmtCond[is.na(train2$BsmtCond)] <- 0
train2$BsmtExposure[is.na(train2$BsmtExposure)] <- 0
train2$BsmtFinType1[is.na(train2$BsmtFinType1)] <- 0
train2$BsmtFinType2[is.na(train2$BsmtFinType2)] <- 0
train2$Electrical[is.na(train2$Electrical)] <- "SBrkr"
train2$FireplaceQu[is.na(train2$FireplaceQu)] <- "None"
train2$GarageType[is.na(train2$GarageType)] <- "None"
train2$GarageYrBlt[is.na(train2$GarageYrBlt)] <- mean(train2$YearBuilt)
train2$GarageFinish[is.na(train2$GarageFinish)] <- "None"
train2$GarageQual[is.na(train2$GarageQual)] <- "None"
train2$GarageCond[is.na(train2$GarageCond)] <- "None"
train2$PoolQC[is.na(train2$PoolQC)] <- "None"
train2$Fence[is.na(train2$Fence)] <- "None"
train2$MiscFeature[is.na(train2$MiscFeature)] <- "None"
na_count <- sapply(train2, function(cnt) sum(length(which(is.na(cnt)))))

```

**Forward Selection**

Forward selection is a variable selection methodology that begins with a constant mean and adds explanatory variables one-by-one until no further additonal predictor variables significantly improve the model's fit. This employess the "F-to-enter" method from the extra-sum-of-squares F-statistic. This was the first method we employed. For this process, we provided the test a starting model with no predictor variables and a model from which terms can be selected, which included all predictor variables available. The process worked forward with selecting one parameter. The suggested model shown in section \ref{appendix:forSelection}.


```{r, echo=F, results='hide'}
# Initial Model - Forward Selection
model2.forward.Start <- lm(log(SalePrice)~1,data = train2)

# All Variables Model - Forward Selection
model2.Allvar <- lm(log(SalePrice) ~ Id + MSSubClass + MSZoning + LotFrontage + LotArea + Street + 
                      Alley + LotShape + LandContour + Utilities + LotConfig + LandSlope +
                      Neighborhood + Condition1 + Condition2 + BldgType + HouseStyle + OverallQual +
                      OverallCond + YearBuilt + YearRemodAdd + RoofStyle + RoofMatl + Exterior1st +
                      Exterior2nd + MasVnrType + MasVnrArea + ExterQual + ExterCond + Foundation +
                      BsmtQual + BsmtCond + BsmtExposure + BsmtFinType1 + BsmtFinSF1 + BsmtFinType2 
                    + BsmtFinSF2 + BsmtUnfSF + TotalBsmtSF + Heating + HeatingQC + CentralAir + 
                      Electrical + `1stFlrSF` + `2ndFlrSF` + LowQualFinSF + GrLivArea + BsmtFullBath
                    + BsmtHalfBath + FullBath + HalfBath + BedroomAbvGr + KitchenAbvGr + KitchenQual
                    + TotRmsAbvGrd + Functional + Fireplaces + FireplaceQu + GarageType +
                      GarageYrBlt + GarageFinish + GarageCars + GarageArea + GarageQual + GarageCond
                    + PavedDrive + WoodDeckSF + OpenPorchSF + EnclosedPorch + `3SsnPorch` +
                      ScreenPorch + PoolArea + PoolQC + Fence + MiscFeature + MiscVal + MoSold +
                      YrSold + SaleType + SaleCondition, data = train2
                    )

#### Forward Selection, first pass
model2.Forward <- stepAIC(model2.forward.Start, direction = "forward", trace = F, scope = formula(model2.Allvar))

summary(model2.Forward)
model2.Forward$anova

##### If needed for display, this is the final model, final pass with Forward Selection#####

#model2.final.Forward <- stepAIC(fitFull.all4.Final, direction = "forward", trace = F, scope = formula(model2.Allvar))
# summary(model2.final.Forward)
# model2.final.Forward$anova

############################## First-pass suggested output from Forward Selection:
final.Forward.Model <- lm(log(SalePrice) ~ OverallQual + GrLivArea + Neighborhood + BsmtFinSF1 +
                            OverallCond + YearBuilt + TotalBsmtSF + GarageCars + MSZoning +
                            SaleCondition + BldgType + Functional + LotArea + KitchenQual +
                            BsmtExposure + CentralAir + Condition1 + ScreenPorch + BsmtFullBath +
                            Heating + Fireplaces + YearRemodAdd + Exterior1st + GarageQual +
                            WoodDeckSF + SaleType + OpenPorchSF + HeatingQC + LotConfig +
                            EnclosedPorch + ExterCond + PoolQC + Foundation + LandSlope +
                            RoofMatl + GarageArea + MasVnrType + HalfBath + PoolArea +
                            `3SsnPorch` + Street + KitchenAbvGr + GarageCond + FullBath +
                            BsmtQual + BsmtFinSF2, data = train2
                          )
```


**Backward Elimination**

Backward elimination is a variable selection methodology that begins with all possible predictor variables and works backward, eliminating variables using all possible combinations until only the best for the fit are provided. This employess the "F-to-remove" method from the extra-sum-of-squares F-statistic. For this process, we provided the test a model with all available predictor variables from which insignificant variables were eliminated. The suggested model shown in section \ref{appendix:backSelection}.
```{r, echo=F, results='hide'}
# Backward Elimination
model2.Backward <- stepAIC(model2.Allvar, direction = "backward", trace = F, scope = formula(model2.forward.Start))

summary(model2.Backward)
model2.Backward$anova

############################## First-pass suggested output from Backward Elimination:
final.Backward.Model <- lm(log(SalePrice) ~ MSZoning + LotArea + Street + LotConfig + LandSlope +
                             Neighborhood + Condition1 + Condition2 + BldgType + OverallQual +
                             OverallCond + YearBuilt + YearRemodAdd + RoofStyle + RoofMatl +
                             Exterior1st + MasVnrType + ExterCond + Foundation + BsmtQual +
                             BsmtCond + BsmtExposure + BsmtFinSF1 + BsmtFinSF2 + BsmtUnfSF +
                             Heating + HeatingQC + CentralAir + `1stFlrSF` + `2ndFlrSF` +
                             LowQualFinSF + BsmtFullBath + FullBath + HalfBath + KitchenAbvGr +
                             KitchenQual + Functional + Fireplaces + GarageCars + GarageArea +
                             GarageQual + GarageCond + WoodDeckSF + OpenPorchSF + EnclosedPorch +
                             `3SsnPorch` + ScreenPorch + PoolArea + PoolQC + SaleType +
                             SaleCondition, data = train2
                           )

##### If needed for display, this is the final model, final validation with Backward Elimination
#model2.final.Backward <- stepAIC(fitFull.all4.Final, direction = "backward", trace = F)
# summary(model2.final.Backward)
# model2.final.Backward$anova
```

**Stepwise Regression**

Stepwise regression is a variable selection methodology that performs one step of forward selection for each step of backward elimination. The steps are repeated, concurrently, until no further predictor variables can be added or removed. This is the third model approach we used. The suggested model shown in section \ref{appendix:stepWSelection}.

```{r, echo=F, results='hide'}
# Stepwise Selection
model2.Stepwise <- stepAIC(model2.Allvar, direction = "both", trace = F)

summary(model2.Stepwise)
model2.Stepwise$anova

############################## First-pass suggested output from Stepwise Regression:
final.Stepwise.Model <- lm(log(SalePrice) ~ MSZoning + LotArea + Street + LotConfig + LandSlope +
                             Neighborhood + Condition1 + Condition2 + BldgType + OverallQual +
                             OverallCond + YearBuilt + YearRemodAdd + RoofStyle + RoofMatl +
                             Exterior1st + MasVnrType + ExterCond + Foundation + BsmtQual +
                             BsmtCond + BsmtExposure + BsmtFinSF1 + BsmtFinSF2 + BsmtUnfSF +
                             Heating + HeatingQC + CentralAir + `1stFlrSF` + `2ndFlrSF` +
                             LowQualFinSF + BsmtFullBath + FullBath + HalfBath + KitchenAbvGr +
                             KitchenQual + Functional + Fireplaces + GarageCars + GarageArea +
                             GarageQual + GarageCond + WoodDeckSF + OpenPorchSF + EnclosedPorch +
                             `3SsnPorch` + ScreenPorch + PoolArea + PoolQC + SaleType +
                             SaleCondition, data = train2
                           )

# If needed for display, this is the final model, final validation with Stepwise Regression
#stepwise.final.model4 <- stepAIC(fitFull.all4.Final, direction = "both", trace = F)
#summary(stepwise.final.model4)
#stepwise.final.model4$anova
```

**Custom Variable Selection** 

To develop the custom model, we employed a combination of a correlation matrix for quantitative data, analysis of the summarization of the suggested model from stepwise selection, and through direct analysis of the pairs plots. As previously mentioned, our final model included 11 linear terms and two interaction terms. We removed all variables suggested to be removed by the stepwise regression and backward elimination tests, then reprocessed the updated models until forward selection, backward elimination, and stepwise regression were in agreeance with respect to the linear terms. Once this trial-and-error process was completed, we added interaction terms based on domain knowledge and re-applied the forward selection, backward elimination, and stepwise regression methods until only significant terms - both linear and interactive - remained. We then used graphical analysis to visually confirm interaction between the interactive terms remaining. The custom model shown in section \ref{appendix:customSelection}.

```{r, echo=F, results='hide'}

#train2.numeric <- dplyr::select_if(train2, is.numeric) %>% data.frame()

#flattenCorrMatrix <- function(cormatrix, pmatrix) {
#  ut <- upper.tri(cormatrix)
# data.frame(
#    row = rownames(cormatrix)[row(cormatrix)[ut]],
#    column = rownames(cormatrix)[col(cormatrix)[ut]],
#    cor  =(cormatrix)[ut],
#    p = pmat[ut]
#    )
#}

#See what variables are correlated with eachother, p-values
#correlation.matrix <- rcorr(as.matrix(train2.numeric))
#corDF <- data.frame(flattenCorrMatrix(correlation.matrix$r, correlation.matrix$P))

#Order the correlation matrix to show the highest correlated
#data.frame(corDF[order(-corDF$cor),])
#quantDataModel <- corDF[which(corDF$cor >= 0.5),]

fitFull.all4.Final <- lm(log(SalePrice) ~ BsmtUnfSF + CentralAir + HalfBath + KitchenQual + Neighborhood + OverallCond + OverallQual + RoofMatl + `1stFlrSF` + `2ndFlrSF` + YearBuilt + MSZoning:Neighborhood + YearBuilt:Neighborhood, data = train2)

######################################################################
###We need the graphs for interactive terms to display interaction####
######################################################################
```

## Model Assumption Assessment


**Custom Model Assumptions**

Based on the below residuals vs. fitted plot, there seems to be strong linearity in the custom model, along with constant variance. Furthermore, while there is one outlier, there is not an egregious violation of any other residuals. The red line very closely approximates the gray dashed line, which furthermore supports the arguement of a well-fit model.

Based on the QQ plot, there is a small level of deviation on the ends of the distribution of the errors, but for the most part, the errors adhere to normality.

In analyzing the standardized residuals scale-location plot, the line has a slight curve, but for the most part is linear, indicating constant variance, or homoscedasticity.

Based on the residuals vs. leverage plot, only two values have high leverage and only one point has a high Cook's distance. However, these are only three data points of over a 1,400 so this is not cause for concern.
```{r, custom-assumptions, echo=F, warning=F, fig.width=5, fig.height=5, fig.align='center', fig.cap='Custom Assumption Plots', out.width = '45%', fig.pos="htbp", fig.show = 'hold'}
#par(mfrow=c(2,2))
#plot(fitFull.all4.Final, main = "Custom Model")

# create plots of residuals
basic.fit.plots(train2, fitFull.all4.Final)
# create leverage / outlier plot
ols_plot_resid_lev(fitFull.all4.Final)
```


**Forward Selection Model Assumptions**

Based on the below residuals vs. fitted plot, there seems to be reasonable linearity in the custom model, along with constant variance. Furthermore, while there are some outliers, there is not an egregious violation of any other residuals. The red line very closely approximates the gray dashed line, which furthermore supports the arguement for a well-fit model.

Based on the QQ plot, there is a some deviation towards the tails of the distribution of the errors, but as with the custom plots, the large majority of errors adhere to normality.

In analyzing the standardized residuals scale-location plot, the line has a moderate curve, indicating some violation of homoscedasticity. However, this is due to a few outliers and as more data is added, the line should straighten out more.

Based on the residuals vs. leverage plot, there are a few more values than the custom model that have high leverage or a moderately large Cook's distance. However, since these are only a few data points, there is not a large cause for concern with this model.

```{r, forward-assumptions, echo=F, warning=F, fig.width=5, fig.height=5, fig.align='center', fig.cap='Forward Selection Assumption PLots', out.width = '50%', fig.pos="htbp"}
par(mfrow=c(2,2))
plot(model2.Forward, main = "Forward Selection Model")
```


**Backward Elimination Model Assumptions**

The backward elimination and stepwise regression models comprised the same parameters. Therefore, they shared the same plots. Compared to the forward selection model, all plots are very similar. Thus, model assumptions are roughly the same.


```{r, backward-selection, echo=F, warning=F, fig.width=5, fig.height=5, fig.align='center', fig.cap='Backward Selection Assumption PLots', out.width = '50%', fig.pos="htbp"}
par(mfrow=c(2,2))
plot(model2.Backward, main = "Backward Elimination Model")
```

**Stepwise Regression model plots**



```{r, stepwise-assimptions, echo=F, warning=F, fig.width=5, fig.height=5, fig.align='center', fig.cap='Stepwise Selection Assumption Plots', out.width = '50%', fig.pos="htbp"}
par(mfrow=c(2,2))
plot(model2.Stepwise, main = "Stepwise Regression Model")
```


## Comparing Competing Models

While the models from forward, backward, and stepwise selection produce higher adjusted $R^2$ values on the training data, the yield much higher errors when applied to the Kaggle test set. These selection methods appear to be overfitting to the training data, thus fail to generalize to the Kaggle test set. Undisputedly, the custom model outperformed the model built strictly on the output of the forward selection, backward elimination, and stepwise regression variable selection procedures when applied to a new dataset.

```{r, echo=F, results='hide', warning=F}
#The final adjusted $R^2$ of our custom model is 0.8867:
R2.custom <- summary(fitFull.all4.Final)$adj.r.squared ##Custom Model
```

```{r, echo=F, results='hide', warning=F}
#The final adjusted $R^2$ of our Forward Selection model is 0.9364:
R2.forward <- summary(model2.Forward)$adj.r.squared
```


```{r, echo=F, results='hide', warning=F}
#The final adjusted $R^2$ of our Backward Elimination model is 0.9367:
R2.backward <- summary(model2.Backward)$adj.r.squared
```


```{r, echo=F, results='hide', warning=F}
#The final adjusted $R^2$ of our Stepwise Regression model is 0.9367:
R2.step <- summary(model2.Stepwise)$adj.r.squared
```

```{r, echo=F, results='hide', warning=F}
#Our cross-validated PRESS statistic output for our custom model is below.

# Set up repeated k-fold cross-validation
train.control2 <- trainControl(method = "cv", number = 10)
# Train the model
model.cv2 <- train(log(SalePrice) ~ BsmtUnfSF + CentralAir + HalfBath + KitchenQual + Neighborhood +
                     OverallCond + OverallQual + RoofMatl + `1stFlrSF` + `2ndFlrSF` + YearBuilt +
                     MSZoning:Neighborhood + YearBuilt:Neighborhood,
                   data = train2,
                   method = 'lm',
                   trControl = train.control2)

# print model summary
model.cv2

# get the CV results
res <- model.cv2$results

# get cross-validated PRESS statistic
PCV.custom <- PRESS.cv(model.cv2)
```

```{r, echo=F, results='hide', warning=F}
#Our cross-validated PRESS statistic output for our Forward Selection model is below.

# Set up repeated k-fold cross-validation
train.control2 <- trainControl(method = "cv", number = 10)
# Train the model
model.cv2 <- train(log(SalePrice) ~ OverallQual + GrLivArea + Neighborhood + BsmtFinSF1 +
                            OverallCond + YearBuilt + TotalBsmtSF + GarageCars + MSZoning +
                            SaleCondition + BldgType + Functional + LotArea + KitchenQual +
                            BsmtExposure + CentralAir + Condition1 + ScreenPorch + BsmtFullBath +
                            Heating + Fireplaces + YearRemodAdd + Exterior1st + GarageQual +
                            WoodDeckSF + SaleType + OpenPorchSF + HeatingQC + LotConfig +
                            EnclosedPorch + ExterCond + PoolQC + Foundation + LandSlope +
                            RoofMatl + GarageArea + MasVnrType + HalfBath + PoolArea +
                            `3SsnPorch` + Street + KitchenAbvGr + GarageCond + FullBath +
                            BsmtQual + BsmtFinSF2,
                   data = train2,
                   method = 'lm',
                   trControl = train.control2)

# print model summary
model.cv2

# get the CV results
res <- model.cv2$results

# get cross-validated PRESS statistic
PCV.forward <- PRESS.cv(model.cv2)
```

```{r, echo=F, results='hide', warning=F}
#Our cross-validated PRESS statistic output for our Backward Elimination model is below.

# Set up repeated k-fold cross-validation
train.control2 <- trainControl(method = "cv", number = 10)
# Train the model
model.cv2 <- train(log(SalePrice) ~ MSZoning + LotArea + Street + LotConfig + LandSlope +
                             Neighborhood + Condition1 + Condition2 + BldgType + OverallQual +
                             OverallCond + YearBuilt + YearRemodAdd + RoofStyle + RoofMatl +
                             Exterior1st + MasVnrType + ExterCond + Foundation + BsmtQual +
                             BsmtCond + BsmtExposure + BsmtFinSF1 + BsmtFinSF2 + BsmtUnfSF +
                             Heating + HeatingQC + CentralAir + `1stFlrSF` + `2ndFlrSF` +
                             LowQualFinSF + BsmtFullBath + FullBath + HalfBath + KitchenAbvGr +
                             KitchenQual + Functional + Fireplaces + GarageCars + GarageArea +
                             GarageQual + GarageCond + WoodDeckSF + OpenPorchSF + EnclosedPorch +
                             `3SsnPorch` + ScreenPorch + PoolArea + PoolQC + SaleType +
                             SaleCondition,
                   data = train2,
                   method = 'lm',
                   trControl = train.control2)

# print model summary
model.cv2

# get the CV results
res <- model.cv2$results

# get cross-validated PRESS statistic
PCV.backward <- PRESS.cv(model.cv2)
```

```{r, echo=F, results='hide', warning=F}

#Our cross-validated PRESS statistic output for our Stepwise Regression model is below.
# Set up repeated k-fold cross-validation
train.control2 <- trainControl(method = "cv", number = 10)
# Train the model
model.cv2 <- train(log(SalePrice) ~ MSZoning + LotArea + Street + LotConfig + LandSlope +
                             Neighborhood + Condition1 + Condition2 + BldgType + OverallQual +
                             OverallCond + YearBuilt + YearRemodAdd + RoofStyle + RoofMatl +
                             Exterior1st + MasVnrType + ExterCond + Foundation + BsmtQual +
                             BsmtCond + BsmtExposure + BsmtFinSF1 + BsmtFinSF2 + BsmtUnfSF +
                             Heating + HeatingQC + CentralAir + `1stFlrSF` + `2ndFlrSF` +
                             LowQualFinSF + BsmtFullBath + FullBath + HalfBath + KitchenAbvGr +
                             KitchenQual + Functional + Fireplaces + GarageCars + GarageArea +
                             GarageQual + GarageCond + WoodDeckSF + OpenPorchSF + EnclosedPorch +
                             `3SsnPorch` + ScreenPorch + PoolArea + PoolQC + SaleType +
                             SaleCondition,
                   data = train2,
                   method = 'lm',
                   trControl = train.control2)

# print model summary
model.cv2

# get the CV results
res <- model.cv2$results

# get cross-validated PRESS statistic
PCV.stepwise <- PRESS.cv(model.cv2)
```

```{r,echo=F, results='hide', warning=F}
#Our custom model achieved a Kaggle score of 0.14494. This was our best score.

## To test in Kaggle, submit the produced "submit" file
test$predicted.log.price <- predict.lm(fitFull.all4.Final, test)
test$predicted.log.price[is.na(test$predicted.log.price)] <- mean(test$predicted.log.price, na.rm = T)

submit <- test %>% mutate(SalePrice = exp(predicted.log.price)) %>% subset(select=c(Id, SalePrice))

write.csv(submit, file = "./kaggle_submission.csv", row.names = F)
```

```{r,echo=F, results='hide', warning=F}
#Our Forward Selection model achieved a Kaggle score of 0.70145

## To test in Kaggle, submit the produced "submit" file
test$predicted.log.price <- predict.lm(final.Forward.Model, test)
test$predicted.log.price[is.na(test$predicted.log.price)] <- mean(test$predicted.log.price, na.rm = T)

submit <- test %>% mutate(SalePrice = exp(predicted.log.price)) %>% subset(select=c(Id, SalePrice))

write.csv(submit, file = "./kaggle_submission.csv", row.names = F)
```

```{r,echo=F, results='hide', warning=F}
#Our Backward Selection model achieved a Kaggle score of 0.73830

## To test in Kaggle, submit the produced "submit" file
test$predicted.log.price <- predict.lm(final.Backward.Model, test)
test$predicted.log.price[is.na(test$predicted.log.price)] <- mean(test$predicted.log.price, na.rm = T)

submit <- test %>% mutate(SalePrice = exp(predicted.log.price)) %>% subset(select=c(Id, SalePrice))

write.csv(submit, file = "./kaggle_submission.csv", row.names = F)
```

```{r,echo=F, results='hide', warning=F}
#Our Stepwise Regression model also achieved a Kaggle score of 0.73830

## To test in Kaggle, submit the produced "submit" file
test$predicted.log.price <- predict.lm(final.Stepwise.Model, test)
test$predicted.log.price[is.na(test$predicted.log.price)] <- mean(test$predicted.log.price, na.rm = T)

submit <- test %>% mutate(SalePrice = exp(predicted.log.price)) %>% subset(select=c(Id, SalePrice))

write.csv(submit, file = "./kaggle_submission.csv", row.names = F)
```


```{r, echo=FALSE}
# print accuracy metrics to md table

kable(data.frame('Model' = c('Custom', 'Forward Selection', 'Backward Selection', 'Stepwise Regression'), 
                 'Kaggle Score'=c(0.14494,0.70145,0.73830,0.73830),
                 'CV Press'=c(PCV.custom, PCV.forward, PCV.backward, PCV.stepwise),
                 'Adjused R Squared'=c(R2.custom, R2.forward, R2.backward, R2.step)),
      "latex", booktabs = T)  %>%
  kable_styling(position = "center")
```

## Conclusion

A short summary of the analysis

# Appendix

## Checking for Linearity in `SalePrice` vs `GrLivArea`

\label{appendix:linearity}

The scatter plot in Figure \@ref(fig:scatter-plot) shows relationship of `SalePrice` vs `GrLivArea` for all three neighborhoods of interest to Century 21. Based on this plot, it does not appear that this relationship meets the assumptions of linear regression, specifically the constant varaince assumption. The response will be transformed to attempt to handle the changing variance.

```{r, scatter-plot, echo=FALSE, fig.width=5, fig.height=5, fig.align='center', fig.cap='Scatter Plot of Sale Price vs Living Room Area', out.width = '45%', fig.pos="htbp"}

# scatter plot of observations from all three neighborhoods
train %>% 
  ggplot(aes(x = (GrLivArea), y = (SalePrice))) +
  geom_point(alpha = 0.3) +
  labs(title = 'Sale Price vs Living Room Area', 
       y = 'Sale Price', x = 'Living Room Area (sq. ft.)')
```

The images below show the scatter plots of log sale price vs living room area (Figure \@ref(fig:scatter-plots)). In the image on the right, the scatter plot is shown for each neighborhood. In the image on the left the observations for all three neighborhoods are included. In all cases, a linear model appears to be reasonable to model this data.

```{r, scatter-plots, echo=FALSE, fig.width=5, fig.height=5, fig.align='center', fig.cap='Scatter Plots of Log of Sale Price vs Living Room Area', out.width = '45%', fig.pos="htbp", fig.show='hold'}

# plots of log of sale price ~ living room area

# create scatter plot for northwest ames
regplot.names <- train %>% filter(Neighborhood == 'NAmes') %>%
  ggplot(aes(x = (GrLivArea), y = log(SalePrice))) +
  geom_point(alpha = 0.3) +
  ylim(10, 13) +
  xlim(0, 3500) +
  labs(subtitle = 'Northwest Ames', 
       y = 'Log of Sale Price', x = 'Living Room Area (sq. ft.)')

# create scatter plot for edwards
regplot.ed <- train %>%
  filter(GrLivArea < 4000) %>%
  filter(Neighborhood == 'Edwards') %>%
  ggplot(aes(x = (GrLivArea), y = log(SalePrice))) +
  geom_point(alpha = 0.3) +
  ylim(10, 13) +
  xlim(0, 3500) +
  labs(subtitle = 'Edwards', 
       y = 'Log of Sale Price', x = 'Living Room Area (sq. ft.)')

# create regression plot for brookside
regplot.brk <- train %>% filter(Neighborhood == 'BrkSide') %>%
  ggplot(aes(x = (GrLivArea), y = log(SalePrice))) +
  geom_point(alpha = 0.3) +
  ylim(10, 13) +
  xlim(0, 3500) +
  labs(subtitle = 'Brook Side', 
       y = 'Log of Sale Price', x = 'Living Room Area (sq. ft.)')

# add the scatter plots for the neighborhood into a single plot
grid.arrange(regplot.names,regplot.ed,regplot.brk, nrow = 2,
             top = 'Regression Plots for Neighborhoods')

# scatter plot of observations from all three neighborhoods
train %>% 
  filter(GrLivArea < 4000) %>%
  ggplot(aes(x = (GrLivArea), y = log(SalePrice))) +
  geom_point(alpha = 0.3) +
  labs(title = 'Log of Sale Price vs Living Room Area', 
       y = 'Log of Sale Price', x = 'Living Room Area (sq. ft.)')
```


## Analysis of Influential points

\label{appendix:infleu-points}

The two outlying observations with living room areas greater than 4000 sq. ft. appear to be from a different distribution than the main dataset. Since these are partial sales, it is possible that the sale prices do not reflect market value. For this reason, we will limit the analysis to properities with less than 3500 sq. ft.  \@ref(fig:infleu-points)

```{r, infleu-points, echo=FALSE, fig.width=5, fig.height=5, fig.align='center', fig.cap='Influential Points', out.width = '50%', fig.pos="htbp"}

# scatter plot of observations from all three neighborhoods with labeling by `SaleCondition`
train %>% ggplot(aes(x = (GrLivArea), y = log(SalePrice))) +
  geom_point(alpha = 0.3) +
  labs(title = 'Log of Sale Price vs Living Room Area', 
       y = 'Log of Sale Price', x = 'Living Room Area (sq. ft.)') +
  geom_text(aes(label = ifelse((log(GrLivArea) > 7.75 & log(SalePrice) > 11) |
                                 (log(SalePrice) > 12.45),
                               SaleCondition, '')), hjust=0, vjust=0)
```

\newpage

## Models Suggested by Automated Selection

### Forward Selection

\label{appendix:forSelection}

The model suggested by forward selection.

```r
log(SalePrice) ~ 
              OverallQual + GrLivArea + Neighborhood + BsmtFinSF1 +
              OverallCond + YearBuilt + TotalBsmtSF + GarageCars + MSZoning +
              SaleCondition + BldgType + Functional + LotArea + KitchenQual +
              BsmtExposure + CentralAir + Condition1 + ScreenPorch + BsmtFullBath +
              Heating + Fireplaces + YearRemodAdd + Exterior1st + GarageQual +
              WoodDeckSF + SaleType + OpenPorchSF + HeatingQC + LotConfig +
              EnclosedPorch + ExterCond + PoolQC + Foundation + LandSlope +
              RoofMatl + GarageArea + MasVnrType + HalfBath + PoolArea +
              `3SsnPorch` + Street + KitchenAbvGr + GarageCond + FullBath +
              BsmtQual + BsmtFinSF2
```

### Backward Selection

\label{appendix:backSelection}

```r
log(SalePrice) ~ 
           MSZoning + LotArea + Street + LotConfig + LandSlope +
           Neighborhood + Condition1 + Condition2 + BldgType + OverallQual +
           OverallCond + YearBuilt + YearRemodAdd + RoofStyle + RoofMatl +
           Exterior1st + MasVnrType + ExterCond + Foundation + BsmtQual +
           BsmtCond + BsmtExposure + BsmtFinSF1 + BsmtFinSF2 + BsmtUnfSF +
           Heating + HeatingQC + CentralAir + `1stFlrSF` + `2ndFlrSF` +
           LowQualFinSF + BsmtFullBath + FullBath + HalfBath + KitchenAbvGr +
           KitchenQual + Functional + Fireplaces + GarageCars + GarageArea +
           GarageQual + GarageCond + WoodDeckSF + OpenPorchSF + EnclosedPorch +
           `3SsnPorch` + ScreenPorch + PoolArea + PoolQC + SaleType +
           SaleCondition
```

\newpage

### Stepwise Selection

\label{appendix:stepWSelection}

```r
log(SalePrice) ~ 
           MSZoning + LotArea + Street + LotConfig + LandSlope +
           Neighborhood + Condition1 + Condition2 + BldgType + OverallQual +
           OverallCond + YearBuilt + YearRemodAdd + RoofStyle + RoofMatl +
           Exterior1st + MasVnrType + ExterCond + Foundation + BsmtQual +
           BsmtCond + BsmtExposure + BsmtFinSF1 + BsmtFinSF2 + BsmtUnfSF +
           Heating + HeatingQC + CentralAir + `1stFlrSF` + `2ndFlrSF` +
           LowQualFinSF + BsmtFullBath + FullBath + HalfBath + KitchenAbvGr +
           KitchenQual + Functional + Fireplaces + GarageCars + GarageArea +
           GarageQual + GarageCond + WoodDeckSF + OpenPorchSF + EnclosedPorch +
           `3SsnPorch` + ScreenPorch + PoolArea + PoolQC + SaleType +
           SaleCondition
```

### Custom Model

\label{appendix:customSelection}

```r
log(SalePrice) ~ 
            BsmtUnfSF + CentralAir + HalfBath + KitchenQual + Neighborhood +
            OverallCond + OverallQual + RoofMatl + `1stFlrSF` + `2ndFlrSF` + 
            YearBuilt + MSZoning:Neighborhood + YearBuilt:Neighborhood
```

## Kaggle Score

The following image shows the result on Kaggle for the custom model.

```{r, kaggel, echo=F, out.width='90%', fig.align='center'}
knitr::include_graphics('./Evidence for Model Score on Kaggle.PNG')
```


\newpage

## R Code For Analysis 1

```r
### Compuational Setup
# libraries
library(knitr)
library(kableExtra)
library(tidyverse)
library(olsrr)
library(gridExtra)
library(caret)
library(multcomp)

# load data
train <- read_csv('./data/train.csv')
test <- read_csv('./data/test.csv')

# set a random seed for repodicibility
set.seed(123)

### Helper Code

#' Print Typical Regression Fit Plots
#' 
#' @description
#' Plots QQ plot of residuals, histogram of residuals,
#' residuals vs predicted values, and studentized 
#' residuals vs predicted values. Depends on tidyverse
#' and gridExtra packages being loaded.
#'
#' @param data The true values corresponding to the input.
#' @param model The predicted/fitted values of the model.
basic.fit.plots <- function(data, model) {
	
	# depends on
	require(tidyverse)
	require(gridExtra)

	# get predicted values
	data$Predicted <- predict(model, data)
	# get residuals
	data$Resid <- model$residuals
	# get studentized residuals
	data$RStudent <- rstudent(model = model)

	# create qqplot of residuals with reference line
	qqplot.resid <- data %>% 
	  ggplot(aes(sample = Resid)) +
	  geom_qq() + geom_qq_line() +
	  labs(subtitle = 'QQ Plot of Residuals',
	       x = 'Theoretical Quantile',
	       y = 'Acutal Quantile')
	
	# create histogram of residuals
	hist.resid <- data %>% 
	  ggplot(aes(x = Resid)) +
	  geom_histogram(bins = 15) + 
	  labs(subtitle = 'Histogram of Residuals',
	       x = 'Residuals',
	       y = 'Count')

	# create scatter plot of residuals vs predicted values
	resid.vs.pred <- data %>% 
	  ggplot(aes(x = Predicted, y = Resid)) +
	  geom_point() +
	  geom_abline(slope = 0) + 
	  labs(subtitle = 'Residuals vs Prediction',
	       x = 'Predicted Value',
	       y = 'Residual')

	# create scatter plot of studentized 
	# residuals vs predicted values
	rStud.vs.pred <- data %>% 
	  ggplot(aes(x = Predicted, y = RStudent)) +
	  geom_point() +
	  geom_abline(slope = 0) + 
  	  geom_abline(slope = 0, intercept = -2) + 
  	  geom_abline(slope = 0, intercept = 2) + 
	  labs(subtitle = 'RStudent vs Prediction',
	       x = 'Predicted Value',
	       y = 'RStudent')
	
	# add all four plots to grid as
	# qqplot           histogram
	# resid vs pred    RStud vs pred
	grid.arrange(qqplot.resid,
		     hist.resid,
		     resid.vs.pred,
		     rStud.vs.pred, 
		     nrow = 2,
		     top = 'Fit Assessment Plots')
}

#' Creates dummy variables (columns) for given column
#'
#' @param data A dataframe.
#' @param column A categorical column in data.
#' @param reference A value in the column to use a reference.
#' @param as.onehot Set to TRUE to use onehot encoding.
#'
get.dummies <- function(data, column, reference, as.onehot = FALSE) {
  # get the levels of the factor in column
  lev <- levels(data[[column]])
  # do not remove reference for onehot encoding
  if (!as.onehot) {
    # remove the reference value
    lev <- lev[lev != reference]
  }
  # add encodings
  for (fct in lev){
    new_col <- paste(column, fct, sep = '_')
    data[new_col] <- as.numeric(data[, column] == fct)
    print(new_col)
  }
  data
}

#' Calculates PRESS from `caret` CV model
#'
#' @param model.cv Calculates press from a model 
#' produced by `caret`
#'
PRESS.cv <- function(model.cv) {
  meanN <- 0
  folds <- model.cv$control$index
  for (i in seq(1:length(folds))){
    meanN <- meanN + length(folds[[i]])
  }
  meanN <- meanN / length(folds)
  meanN * ((model.cv$results$RMSE)^2)
}

### plots of log of sale price ~ living room area

# create scatter plot for northwest ames
regplot.names <- train %>% filter(Neighborhood == 'NAmes') %>%
  ggplot(aes(x = (GrLivArea), y = log(SalePrice))) +
  geom_point(alpha = 0.3) +
  ylim(10, 13) +
  xlim(0, 3500) +
  labs(subtitle = 'Northwest Ames', 
       y = 'Log of Sale Price', x = 'Living Room Area (sq. ft.)')

# create scatter plot for edwards
regplot.ed <- train %>%
  filter(GrLivArea < 4000) %>%
  filter(Neighborhood == 'Edwards') %>%
  ggplot(aes(x = (GrLivArea), y = log(SalePrice))) +
  geom_point(alpha = 0.3) +
  ylim(10, 13) +
  xlim(0, 3500) +
  labs(subtitle = 'Edwards', 
       y = 'Log of Sale Price', x = 'Living Room Area (sq. ft.)')

# create regression plot for brookside
regplot.brk <- train %>% filter(Neighborhood == 'BrkSide') %>%
  ggplot(aes(x = (GrLivArea), y = log(SalePrice))) +
  geom_point(alpha = 0.3) +
  ylim(10, 13) +
  xlim(0, 3500) +
  labs(subtitle = 'Brook Side', 
       y = 'Log of Sale Price', x = 'Living Room Area (sq. ft.)')

# add the scatter plots for the neighborhood into a single plot
grid.arrange(regplot.names,regplot.ed,regplot.brk, nrow = 2,
             top = 'Regression Plots for Neighborhoods')

# scatter plot of observations from all three neighborhoods
train %>% 
  filter(GrLivArea < 4000) %>%
  ggplot(aes(x = (GrLivArea), y = log(SalePrice))) +
  geom_point(alpha = 0.3) +
  labs(title = 'Log of Sale Price vs Living Room Area', 
       y = 'Log of Sale Price', x = 'Living Room Area (sq. ft.)')

### Filter data for analysis 1

train <- train %>% 
  filter(Neighborhood %in% c("Edwards", "BrkSide", "NAmes"))
train$Neighborhood <- as.factor(train$Neighborhood)

# create dummy variables with Neighborhood == 'Edwards' as reference
train <- get.dummies(train, "Neighborhood", reference = 'Edwards')

# remove suspect points from training data
train.mod <- train %>% filter(GrLivArea < 4000)

#### Extra Sum of Squares

# full model formula
model.formula = log(SalePrice) ~ (GrLivArea) + 
     Neighborhood_BrkSide + 
     Neighborhood_NAmes +
     (GrLivArea) * Neighborhood_BrkSide + 
     (GrLivArea) * Neighborhood_NAmes
# reduced model formula
model.reduced.formula = log(SalePrice) ~ (GrLivArea) + 
     Neighborhood_BrkSide + 
     Neighborhood_NAmes

# fit models
model <- lm(formula = model.formula, data = train.mod)
model.reduced <- lm(formula = model.reduced.formula, data = train.mod)
# ESS test on models
anova(model.reduced, model)

### Assessment plots

# create plots of residuals
basic.fit.plots(train.mod, model)
# create leverage / outlier plot
ols_plot_resid_lev(model)

### cross validation

## cross validate the full model

# Set up repeated k-fold cross-validation
train.control <- trainControl(method = "cv", number = 10)
# Train the model
model.cv <- train(model.formula, 
                    data = train.mod,
                    method = 'lm',
                    trControl = train.control)
# print model summary
model.cv

# get the CV results
res <- model.cv$results

# get cross-validated PRESS statistic
PCV <- PRESS.cv(model.cv)

## cross validate the reduced model

# Set up repeated k-fold cross-validation
train.control <- trainControl(method = "cv", number = 10)
# Train the model
model.reduced.cv <- train(model.reduced.formula, 
                    data = train.mod,
                    method = 'lm',
                    trControl = train.control)
# print model summary
model.reduced.cv

# get the CV results
res.red <- model.reduced.cv$results

# get cross-validated PRESS statistic
PCV.red <- PRESS.cv(model.reduced.cv)

# print accuracy metrics to md table
kable(data.frame('Model' = c('Full Model', 'Reduced Model'), 
                 'RMSE'=c(res$RMSE, res.red$RMSE),
                 'CV Press'=c(PCV, PCV.red),
                 'Adjused R Squared'=c(res$Rsquared, res.red$Rsquared)),
      "latex", booktabs = T)  %>%
  kable_styling(position = "center")

### get the parameters from the CV'ed model

# extract the model estimates from the model summary
sm <- summary(model)
sm.coe <- sm$coefficients
# get the CIs for the coefficients
model.conf <- confint(model)

# print model estimates to md / latex table
# extract the params and put into a dataframe
kable(data.frame('Parameter' = c('Intercept', 'GrLivArea', 
                                 'Neighborhood_BrkSide', 'Neighborhood_NAmes', 
                                 'GrLivArea:Neighborhood_BrkSide', 
                                 'GrLivArea:Neighborhood_NAmes '), 
                 'Estimate'=c(sm.coe[[1]],sm.coe[[2]],sm.coe[[3]],
                              sm.coe[[4]],sm.coe[[5]],sm.coe[[6]]),
                 'CI Lower' = c(model.conf[[1]],model.conf[[2]],model.conf[[3]],
                                model.conf[[4]],model.conf[[5]],model.conf[[6]]),
                 'CI Upper' = c(model.conf[[1,2]],model.conf[[1,2]],model.conf[[3,2]],
                                model.conf[[4,2]],model.conf[[5,2]],model.conf[[6,2]])),
      "latex", booktabs = T)  %>%
  kable_styling(position = "center")

# summary of model to get overall test
summary(lm(model.formula, data = train.mod))

## Calculate CIs of slopes not in standard table

# get CI for Northwest Ames
confint(glht(model, linfct = "GrLivArea + GrLivArea:Neighborhood_NAmes = 1"))

# get CI for Brookside
confint(glht(model, linfct = "GrLivArea + GrLivArea:Neighborhood_BrkSide = 1"))

```

\newpage

## R Code For Analysis 2

Include "well commented" `code` in the appendex!

```r
train %>% ggplot(aes(x = (GrLivArea), y = log(SalePrice))) +
  geom_point(alpha = 0.3) +
  labs(title = 'Log of Sale Price vs Living Room Area', 
       y = 'Log of Sale Price', x = 'Living Room Area') +
  geom_text(aes(label = ifelse((log(GrLivArea) > 7.75 & log(SalePrice) > 11) |
                                 (log(SalePrice) > 12.45),
                               SaleCondition, '')), hjust=0, vjust=0)
```

\newpage

# References
